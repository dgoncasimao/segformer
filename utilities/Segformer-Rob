import os
import numpy as np
from PIL import Image
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import pytorch_lightning as pl
from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor
import matplotlib.pyplot as plt

# === Configurations ===
IMAGE_DIR = 'C:/Users/DSBG-Public/segformer/dataset/train/images'
MASK_DIR = 'C:/Users/DSBG-Public/segformer/dataset/train/masks'
TEST_IMAGE_DIR = 'C:/Users/DSBG-Public/segformer/dataset/test/images'
TEST_MASK_DIR = 'C:/Users/DSBG-Public/segformer/dataset/test/masks'
NUM_CLASSES = 2

# === Dataset ===
class SegmentationDataset(Dataset):
    def __init__(self, image_dir, mask_dir, feature_extractor, resize=(512, 512)):
        self.image_dir = image_dir
        self.mask_dir = mask_dir
        self.resize = resize
        self.feature_extractor = feature_extractor
        self.images = sorted(os.listdir(image_dir))
        self.masks = sorted(os.listdir(mask_dir))

    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        image = Image.open(os.path.join(self.image_dir, self.images[idx])).convert("RGB")
        mask = Image.open(os.path.join(self.mask_dir, self.masks[idx])).convert("L")
        image = image.resize(self.resize)
        mask = mask.resize(self.resize)
        
        encoding = self.feature_extractor(images=image, return_tensors="pt")
        pixel_values = encoding['pixel_values'].squeeze(0)
        
        mask = np.array(mask)
        mask = (mask > 127).astype(np.uint8)
        mask = np.clip(mask, 0, NUM_CLASSES - 1)
        mask = torch.tensor(mask, dtype=torch.long)
        
        encoding['labels'] = mask
        encoding['pixel_values'] = pixel_values
        return encoding

# === Loss Functions & Metrics ===
def tversky_loss(logits, labels, alpha=0.7, beta=0.3, smooth=1e-6):
    num_classes = logits.shape[1]
    labels_one_hot = F.one_hot(labels, num_classes).permute(0, 3, 1, 2).float()
    probs = torch.softmax(logits, dim=1)
    tversky_loss_value = 0.0
    for c in range(num_classes):
        prob_c = probs[:, c, :, :]
        true_c = labels_one_hot[:, c, :, :]
        TP = (prob_c * true_c).sum(dim=(1, 2))
        FP = (prob_c * (1 - true_c)).sum(dim=(1, 2))
        FN = ((1 - prob_c) * true_c).sum(dim=(1, 2))
        tversky_index = (TP + smooth) / (TP + alpha * FP + beta * FN + smooth)
        tversky_loss_value += (1 - tversky_index).mean()
    tversky_loss_value /= num_classes
    return tversky_loss_value

def combined_loss(logits, labels, weight_ce=1.0, weight_tv=1.0, smooth=1e-6, alpha=0.7, beta=0.3):
    ce_loss = torch.nn.CrossEntropyLoss()(logits, labels)
    tv_loss = tversky_loss(logits, labels, alpha=alpha, beta=beta, smooth=smooth)
    return weight_ce * ce_loss + weight_tv * tv_loss

def compute_pixel_accuracy(preds, labels):
    preds = preds.flatten()
    labels = labels.flatten()
    correct_pixels = (preds == labels).sum().item()
    return correct_pixels / labels.numel()

def compute_dice_coefficient(preds, labels, num_classes=2):
    dice_scores = []
    for i in range(num_classes):
        intersection = torch.sum((preds == i) & (labels == i))
        total_pixels = torch.sum((preds == i) + (labels == i))
        dice = (2 * intersection.float()) / (total_pixels.float() + 1e-10)
        dice_scores.append(dice.item())
    return sum(dice_scores) / len(dice_scores)

def compute_iou(preds, labels, num_classes):
    preds = preds.flatten()
    labels = labels.flatten()
    iou_list = []
    for i in range(num_classes):
        intersection = torch.sum((preds == i) & (labels == i))
        union = torch.sum((preds == i) | (labels == i))
        iou = intersection.float() / (union.float() + 1e-10)
        iou_list.append(iou.item())
    return sum(iou_list) / len(iou_list)

# === PyTorch Lightning Module ===
class LitSegformer(pl.LightningModule):
    def __init__(self, num_labels=NUM_CLASSES, learning_rate=5e-5, weight_decay=0.01):
        super().__init__()
        self.save_hyperparameters()
        self.model = SegformerForSemanticSegmentation.from_pretrained(
            "nvidia/segformer-b0-finetuned-ade-512-512",
            num_labels=num_labels,
            ignore_mismatched_sizes=True
        )
    
    def forward(self, pixel_values):
        return self.model(pixel_values=pixel_values)
    
    def training_step(self, batch, batch_idx):
        labels = batch["labels"]
        pixel_values = batch["pixel_values"]
        outputs = self.model(pixel_values=pixel_values)
        logits = outputs.logits
        # Resize labels to match logits spatial dimensions if necessary.
        if labels.shape[-2:] != logits.shape[-2:]:
            labels = labels.unsqueeze(1).float()
            labels = F.interpolate(labels, size=logits.shape[-2:], mode="nearest")
            labels = labels.squeeze(1).long()
        loss = combined_loss(logits, labels)
        self.log("train_loss", loss, prog_bar=True)
        return loss

    def validation_step(self, batch, batch_idx):
        labels = batch["labels"]
        pixel_values = batch["pixel_values"]
        outputs = self.model(pixel_values=pixel_values)
        logits = outputs.logits
        if labels.shape[-2:] != logits.shape[-2:]:
            labels = labels.unsqueeze(1).float()
            labels = F.interpolate(labels, size=logits.shape[-2:], mode="nearest")
            labels = labels.squeeze(1).long()
        loss = combined_loss(logits, labels)
        preds = torch.argmax(logits, dim=1)
        acc = compute_pixel_accuracy(preds, labels)
        dice = compute_dice_coefficient(preds, labels, num_classes=NUM_CLASSES)
        iou = compute_iou(preds, labels, num_classes=NUM_CLASSES)
        self.log("val_loss", loss, prog_bar=True)
        self.log("val_acc", acc, prog_bar=True)
        self.log("val_dice", dice, prog_bar=True)
        self.log("val_iou", iou, prog_bar=True)
        return {"val_loss": loss, "val_acc": acc, "val_dice": dice, "val_iou": iou}

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.parameters(), 
            lr=self.hparams.learning_rate, 
            weight_decay=self.hparams.weight_decay
        )
        return optimizer

# === Visualization Callback ===
class ImagePredictionCallback(pl.Callback):
    def __init__(self, val_dataloader, num_images=3):
        super().__init__()
        self.val_dataloader = val_dataloader
        self.num_images = num_images

    def on_validation_epoch_end(self, trainer, pl_module):
        # Get one batch from the validation dataloader
        batch = next(iter(self.val_dataloader))
        pixel_values = batch["pixel_values"]
        labels = batch["labels"]
        device = pl_module.device
        pl_module.model.eval()
        with torch.no_grad():
            outputs = pl_module.model(pixel_values=pixel_values.to(device))
        logits = outputs.logits
        preds = torch.argmax(logits, dim=1)
        
        # Upsample predictions to match input image size if needed
        upsampled_preds = []
        for i in range(self.num_images):
            pred = preds[i].unsqueeze(0).unsqueeze(0).float()  # [1,1,H',W']
            up_pred = F.interpolate(pred, size=pixel_values[i].shape[1:], mode='nearest')
            upsampled_preds.append(up_pred.squeeze().cpu().numpy().astype(np.uint8))
        
        # Convert images from tensor to numpy arrays for plotting
        images = [pixel_values[i].permute(1, 2, 0).cpu().numpy() for i in range(self.num_images)]
        gt_masks = [labels[i].cpu().numpy() for i in range(self.num_images)]
        
        # Plot input image, predicted mask, and ground truth side by side
        fig, axs = plt.subplots(self.num_images, 3, figsize=(12, 4 * self.num_images))
        if self.num_images == 1:
            axs = np.expand_dims(axs, axis=0)
        for i in range(self.num_images):
            axs[i, 0].imshow(images[i])
            axs[i, 0].set_title("Input Image")
            axs[i, 0].axis('off')
            
            axs[i, 1].imshow(upsampled_preds[i], cmap='gray')
            axs[i, 1].set_title("Predicted Mask")
            axs[i, 1].axis('off')
            
            axs[i, 2].imshow(gt_masks[i], cmap='gray')
            axs[i, 2].set_title("Ground Truth")
            axs[i, 2].axis('off')
        plt.tight_layout()
        plt.show()

# === Data Setup & Training ===
if __name__ == "__main__":
    feature_extractor = SegformerImageProcessor.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512")
    train_dataset = SegmentationDataset(IMAGE_DIR, MASK_DIR, feature_extractor)
    val_dataset = SegmentationDataset(TEST_IMAGE_DIR, TEST_MASK_DIR, feature_extractor)
    
    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=3, prefetch_factor=8)
    val_dataloader = DataLoader(val_dataset, batch_size=4,  num_workers=3, prefetch_factor=8)
    
    model = LitSegformer(num_labels=NUM_CLASSES)
    
    # Add the visualization callback to display predicted masks after validation
    image_callback = ImagePredictionCallback(val_dataloader, num_images=3)
    
    trainer = pl.Trainer(
        max_epochs=10,
        log_every_n_steps=10,
        callbacks=[image_callback]
    )
    
    trainer.fit(model, train_dataloader, val_dataloader)