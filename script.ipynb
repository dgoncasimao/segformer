{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2d21c2",
   "metadata": {},
   "source": [
    "\n",
    "# Model to mask Vastus Lateralis\n",
    "\n",
    "1) Provide input video file \n",
    "2) Let the model run \n",
    "3) Once it has finished the predicted video will be in the same folder as the input \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95701770",
   "metadata": {},
   "source": [
    "\n",
    "## How to Use \n",
    "1. **Install dependencies** run the next cell    \n",
    "2. **Set hyperparameters** choose to keep defaults or change the parameters.  \n",
    "3. **Provide your input video file** insert the path to your video. \n",
    "4. **Run the rest of the cells** click on \"execute cell and below\"\n",
    "5. **Outputs are saved alongside the input file**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381d9259",
   "metadata": {},
   "source": [
    "## Step 1: Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d31dc588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Install libs (run once)\n",
    "%pip install -q numpy tifffile pynrrd ipywidgets torch torchvision torchaudio transformers pytorch_lightning pillow albumentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8f2ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\ProgramData\\anaconda3\\envs\\segformer\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\segformer\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\segformer\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\segformer\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\segformer\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\utils.py\", line 71, in preserve_context\n",
      "    return await f(*args, **kwargs)\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\DSBG-Public\\AppData\\Local\\Temp\\ipykernel_10780\\3342416775.py\", line 9, in <module>\n",
      "    import torch\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\segformer\\lib\\site-packages\\torch\\__init__.py\", line 1382, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\segformer\\lib\\site-packages\\torch\\functional.py\", line 7, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\segformer\\lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\segformer\\lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\segformer\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\ProgramData\\anaconda3\\envs\\segformer\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\b\\abs_2112s1s0to\\croot\\pytorch-select_1700158736573\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "c:\\ProgramData\\anaconda3\\envs\\segformer\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "import nrrd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor, SegformerFeatureExtractor\n",
    "\n",
    "from ipywidgets import VBox, HBox, Text, Button, Checkbox, Dropdown, IntText, FloatText, HTML, Accordion, Layout\n",
    "from IPython.display import display\n",
    "\n",
    "def info(msg): print(f\"[INFO] {msg}\")\n",
    "def warn(msg): print(f\"[WARN] {msg}\")\n",
    "def ok(msg):   print(f\"[OK]   {msg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca794f0e",
   "metadata": {},
   "source": [
    "## Step 2: Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d98ffde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5396cf2ed0144187aaf9daad865c097c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HBox(children=(Dropdown(description='backend', options=('hf', 'pl'), value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "HP: Dict[str, Any] = {\n",
    "    # Converter + IO\n",
    "    \"use_internal_nrrd_to_tif\": True,\n",
    "    \"tif_subdir\": \"tif_slices\",\n",
    "    \"mask_subdir\": \"pred_masks\",\n",
    "    \"pred_mask_subdir\": \"pred_masks_model\", ,
    "    \"also_save_separate\": True,  \n",
    "    \"overwrite_outputs\": True,\n",
    "    \"save_compressed_nrrd\": True,\n",
    "    \"make_single_nrrd\": True,\n",
    "    \"also_save_separate\": False,\n",
    "\n",
    "    # Model\n",
    "    \"inference_backend\": \"hf\",   # \"hf\" (Hugging Face) or \"pl\" (PyTorch Lightning)\n",
    "    \"hf_model_dir\": \"C:/Users/DSBG-Public/segformer/segformer-1/segformer_export_hf\",          # e.g., a local dir with config+weights OR a HF repo id\n",
    "    \"pl_ckpt_path\": \"\",          # e.g., /path/to/lightning/checkpoints/best.ckpt\n",
    "    \"target_class_idx\": 1,       # used if model has multiple classes; this is the positive class to extract\n",
    "    \"threshold\": 0.5,            # used if model produces single-channel logits (sigmoid) or after softmax prob for target_class\n",
    "    \"batch_size\": 4,\n",
    "    \"resize_to\": 512,            # if >0, resize (H,W) to this square size for model; output is resized back to original\n",
    "\n",
    "    # Preprocessing\n",
    "    \"normalize\": True,           # simple 0-1 normalization\n",
    "}\n",
    "# Quick widget\n",
    "w_backend  = Dropdown(options=[\"hf\",\"pl\"], value=HP[\"inference_backend\"], description=\"backend\")\n",
    "w_hf_dir   = Text(value=HP[\"hf_model_dir\"], description=\"hf_model_dir\", layout=Layout(width=\"60%\"))\n",
    "w_pl_ckpt  = Text(value=HP[\"pl_ckpt_path\"], description=\"pl_ckpt_path\", layout=Layout(width=\"60%\"))\n",
    "w_tclass   = IntText(value=HP[\"target_class_idx\"], description=\"target_class_idx\")\n",
    "w_thr      = FloatText(value=HP[\"threshold\"], description=\"threshold\")\n",
    "w_bs       = IntText(value=HP[\"batch_size\"], description=\"batch_size\")\n",
    "w_resize   = IntText(value=HP[\"resize_to\"], description=\"resize_to\")\n",
    "w_overwr   = Checkbox(value=HP[\"overwrite_outputs\"], description=\"overwrite_outputs\")\n",
    "w_single   = Checkbox(value=HP[\"make_single_nrrd\"], description=\"make_single_nrrd\")\n",
    "w_sep      = Checkbox(value=HP[\"also_save_separate\"], description=\"also_save_separate\")\n",
    "\n",
    "def _apply(_):\n",
    "    HP.update({\n",
    "        \"inference_backend\": w_backend.value,\n",
    "        \"hf_model_dir\": w_hf_dir.value.strip(),\n",
    "        \"pl_ckpt_path\": w_pl_ckpt.value.strip(),\n",
    "        \"target_class_idx\": int(w_tclass.value),\n",
    "        \"threshold\": float(w_thr.value),\n",
    "        \"batch_size\": int(w_bs.value),\n",
    "        \"resize_to\": int(w_resize.value),\n",
    "        \"overwrite_outputs\": w_overwr.value,\n",
    "        \"make_single_nrrd\": w_single.value,\n",
    "        \"also_save_separate\": w_sep.value,\n",
    "        \"max_epochs\": 50,          \n",
    "        \"aug_preset\": \"basic\",     \n",
    "        \"aug_count\": 2,            \n",
    "        \"aug_seed\": 42,            \n",
    "    })\n",
    "    ok(\"Hyperparameters updated.\")\n",
    "btn = Button(description=\"Apply\", button_style=\"primary\")\n",
    "btn.on_click(_apply)\n",
    "display(Accordion(children=[\n",
    "    VBox([\n",
    "        HBox([w_backend, w_bs, w_resize]),\n",
    "        HBox([w_tclass, w_thr]),\n",
    "        w_hf_dir, w_pl_ckpt,\n",
    "        HBox([w_overwr, w_single, w_sep]),\n",
    "        btn\n",
    "    ])\n",
    "], titles=(\"Hyperparameters\",)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d783452",
   "metadata": {},
   "source": [
    "## Step 3: Input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ee0848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f2fe10a4eb482da727181c7fabc105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='', description='Input .nrrd', layout=Layout(width='80%'), placeholde…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_path = Text(description=\"Input .nrrd\", placeholder=\"/path/to/volume.nrrd\", layout=Layout(width=\"80%\"))\n",
    "btn_val = Button(description=\"Validate Path\", button_style=\"primary\")\n",
    "lbl_val = HTML()\n",
    "def _val(_):\n",
    "    p = Path(in_path.value).expanduser().resolve()\n",
    "    lbl_val.value = f\"<b style='color:green'>OK</b>: {p}\" if p.exists() and p.suffix.lower()==\".nrrd\" else f\"<b style='color:#b00'>Invalid</b>: {p}\"\n",
    "btn_val.on_click(_val)\n",
    "display(VBox([HBox([in_path, btn_val]), lbl_val]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7193feb",
   "metadata": {},
   "source": [
    "## Step 3.1: Create masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bf5b587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d5eb68b7e84c13b92907e345e27cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(button_style='primary', description='Run Step 3 (masked+cropped)', style=ButtonStyle()),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def crop_image_get_bounds(image, threshold):\n",
    "    rows = np.any(image > threshold, axis=1)\n",
    "    cols = np.any(image > threshold, axis=0)\n",
    "    if not np.any(rows) or not np.any(cols):\n",
    "        return None\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    return rmin, rmax, cmin, cmax\n",
    "\n",
    "def apply_crop(image, bounds):\n",
    "    if bounds is None:\n",
    "        return None\n",
    "    rmin, rmax, cmin, cmax = bounds\n",
    "    return image[rmin:rmax+1, cmin:cmax+1]\n",
    "\n",
    "def resize_with_padding(image, target_size=(512, 512)):\n",
    "    old_h, old_w = image.shape[:2]\n",
    "    tgt_h, tgt_w = target_size\n",
    "    delta_w = max(tgt_w - old_w, 0)\n",
    "    delta_h = max(tgt_h - old_h, 0)\n",
    "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "    return np.pad(image, ((top, bottom), (left, right)), mode='constant', constant_values=0)\n",
    "\n",
    "def export_to_tif_with_threshold(volume_data, mask_data, out_vol_dir, out_mask_dir, threshold=10, pad_size=1500):\n",
    "    exported_count = 0\n",
    "    if volume_data.shape[0] != mask_data.shape[0]:\n",
    "        # Try to detect if the data is (Y,X,Z)\n",
    "        if volume_data.ndim == 3 and volume_data.shape[2] == mask_data.shape[2]:\n",
    "            volume_data = np.transpose(volume_data, (2,0,1))\n",
    "            mask_data = np.transpose(mask_data, (2,0,1))\n",
    "    assert volume_data.shape == mask_data.shape, f\"Volume/mask shapes differ: {volume_data.shape} vs {mask_data.shape}\"\n",
    "    Z = volume_data.shape[0]\n",
    "\n",
    "    out_vol_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_mask_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i in range(Z):\n",
    "        slice_volume = volume_data[i]\n",
    "        slice_mask = mask_data[i]\n",
    "\n",
    "        masked_volume = slice_volume * slice_mask\n",
    "        if not np.any(masked_volume > threshold):\n",
    "            continue\n",
    "\n",
    "        bounds = crop_image_get_bounds(slice_volume, threshold)\n",
    "        cropped_volume = apply_crop(slice_volume, bounds)\n",
    "        cropped_mask = apply_crop(slice_mask, bounds)\n",
    "        if cropped_volume is None or cropped_mask is None:\n",
    "            continue\n",
    "\n",
    "        volume_resized = resize_with_padding(cropped_volume, target_size=(pad_size, pad_size))\n",
    "        mask_resized = resize_with_padding(cropped_mask, target_size=(pad_size, pad_size))\n",
    "\n",
    "        vpath = out_vol_dir / f\"frame_{exported_count:05d}.tif\"\n",
    "        mpath = out_mask_dir / f\"frame_{exported_count:05d}.tif\"\n",
    "        Image.fromarray(volume_resized.astype(np.uint16 if volume_resized.max()>255 else np.uint8)).save(vpath)\n",
    "        Image.fromarray((mask_resized>0).astype(np.uint8)*255).save(mpath)\n",
    "        exported_count += 1\n",
    "    return exported_count\n",
    "\n",
    "def _infer_volume_and_mask(vol):\n",
    "    # Return (vol3d, mask3d).\n",
    "    if vol.ndim == 4 and vol.shape[0] == 2:\n",
    "        img = vol[0]\n",
    "        msk = vol[1]\n",
    "        return img, msk\n",
    "    if vol.ndim == 3:\n",
    "        return vol, None\n",
    "    raise ValueError(f\"Unsupported NRRD shape: {vol.shape}\")\n",
    "\n",
    "def internal_nrrd_to_tifs_masked(nrrd_path: Path, out_dir_images: Path, out_dir_masks: Path) -> int:\n",
    "    data, header = nrrd.read(str(nrrd_path))\n",
    "    data = np.asarray(data)\n",
    "    img, msk = _infer_volume_and_mask(data)\n",
    "    if msk is None:\n",
    "        # try sibling masks\n",
    "        p = nrrd_path\n",
    "        cands = [p.with_name(p.stem + \"_masks.nrrd\"),\n",
    "                 p.with_name(p.stem + \"_mask.nrrd\")]\n",
    "        for c in cands:\n",
    "            if c.exists():\n",
    "                msk, _ = nrrd.read(str(c))\n",
    "                break\n",
    "    if msk is None and HP.get(\"strict_require_mask\", False):\n",
    "        raise RuntimeError(\"No mask volume provided/found and strict_require_mask=True\")\n",
    "    if msk is None:\n",
    "        # Fallback: create full-ones mask \n",
    "        msk = np.ones_like(img)\n",
    "\n",
    "    # Ensure shapes are (Z,Y,X)\n",
    "    if img.ndim == 3 and img.shape[0] != msk.shape[0]:\n",
    "        if img.shape[2] == msk.shape[2]:\n",
    "            img = np.transpose(img, (2,0,1))\n",
    "            msk = np.transpose(msk, (2,0,1))\n",
    "\n",
    "    n = export_to_tif_with_threshold(\n",
    "        img, msk, out_dir_images, out_dir_masks,\n",
    "        threshold=int(HP.get(\"crop_threshold\", 10)),\n",
    "        pad_size=int(HP.get(\"pad_size\", 1500))\n",
    "    )\n",
    "    return n\n",
    "\n",
    "def do_step3():\n",
    "    p = Path(in_path.value).expanduser().resolve()\n",
    "    assert p.exists() and p.suffix.lower()==\".nrrd\", \"Set a valid .nrrd path.\"\n",
    "    out_root = p.parent\n",
    "    tif_dir  = out_root / HP[\"tif_subdir\"]\n",
    "    gt_dir   = out_root / HP[\"mask_subdir\"]\n",
    "    if not HP[\"overwrite_outputs\"] and (any(tif_dir.glob(\"*.tif\")) or any(gt_dir.glob(\"*.tif\"))):\n",
    "        warn(\"Output dirs not empty; skipping due to overwrite_outputs=False\")\n",
    "        return tif_dir\n",
    "    n = internal_nrrd_to_tifs_masked(p, tif_dir, gt_dir)\n",
    "    ok(f\"Wrote {n} TIF pairs to {tif_dir} and {gt_dir}\")\n",
    "    return tif_dir\n",
    "\n",
    "b3 = Button(description=\"Run Step 3 (masked+cropped)\", button_style=\"primary\")\n",
    "l3 = HTML()\n",
    "def _b3(_):\n",
    "    try:\n",
    "        out = do_step3()\n",
    "        l3.value = f\"Done: <code>{out}</code>\"\n",
    "    except Exception as e:\n",
    "        l3.value = f\"<span style='color:#b00'>Error: {e}</span>\"\n",
    "b3.on_click(_b3)\n",
    "display(VBox([b3, l3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b26ea",
   "metadata": {},
   "source": [
    "# Step 3.2: Dataset Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c9c4968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b0d31fbb1b4023b943f9a1798de186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(button_style='primary', description='Run Step 3.1: Augment Dataset', style=ButtonStyle()…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "def _build_augs(preset: str) -> A.Compose:\n",
    "    if preset == \"none\":\n",
    "        return A.Compose([], is_check_shapes=False)\n",
    "    if preset == \"strong\":\n",
    "        return A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.2),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.06, scale_limit=0.15, rotate_limit=25, border_mode=0, p=0.7),\n",
    "            A.ElasticTransform(alpha=20, sigma=4, alpha_affine=10, border_mode=0, p=0.3),\n",
    "            A.GaussNoise(var_limit=(5.0, 20.0), p=0.3),\n",
    "            A.RandomBrightnessContrast(p=0.4),\n",
    "        ], is_check_shapes=False)\n",
    "    # basic\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.3),\n",
    "        A.ShiftScaleRotate(shift_limit=0.03, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.25),\n",
    "    ], is_check_shapes=False)\n",
    "\n",
    "def run_augmentation():\n",
    "    p = Path(in_path.value).expanduser().resolve()\n",
    "    img_dir = p.parent / HP[\"tif_subdir\"]       \n",
    "    msk_dir = p.parent / HP[\"mask_subdir\"]      \n",
    "    assert img_dir.exists() and msk_dir.exists(), \"Run Step 3 first to create paired TIFs.\"\n",
    "\n",
    "    out_img = p.parent / f\"{HP['tif_subdir']}_aug\"\n",
    "    out_msk = p.parent / f\"{HP['mask_subdir']}_aug\"\n",
    "    out_img.mkdir(parents=True, exist_ok=True)\n",
    "    out_msk.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    preset = HP.get(\"aug_preset\", \"basic\")\n",
    "    copies = int(HP.get(\"aug_count\", 2))\n",
    "    seed = int(HP.get(\"aug_seed\", 42))\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    t = _build_augs(preset)\n",
    "    pairs = sorted(img_dir.glob(\"frame_*.tif\"))\n",
    "    assert pairs, f\"No frames found in {img_dir}\"\n",
    "\n",
    "    written = 0\n",
    "    for img_path in pairs:\n",
    "        m_path = (msk_dir / img_path.name)\n",
    "        if not m_path.exists():\n",
    "            continue\n",
    "        img = tiff.imread(img_path)\n",
    "        msk = tiff.imread(m_path)\n",
    "\n",
    "        # ensure 2D\n",
    "        if img.ndim == 3:\n",
    "            img = img.squeeze()\n",
    "        if msk.ndim == 3:\n",
    "            msk = msk.squeeze()\n",
    "\n",
    "        for k in range(copies):\n",
    "            aug = t(image=img, mask=msk)\n",
    "            ai, am = aug[\"image\"], aug[\"mask\"]\n",
    "\n",
    "            # keep dtype-friendly saves\n",
    "            ai_save = ai.astype(np.uint16 if ai.max() > 255 else np.uint8)\n",
    "            am_save = (am > 0).astype(np.uint8) * 255\n",
    "\n",
    "            stem = img_path.stem  # frame_00000\n",
    "            tiff.imwrite(out_img / f\"{stem}_aug{k:02d}.tif\", ai_save)\n",
    "            tiff.imwrite(out_msk / f\"{stem}_aug{k:02d}.tif\", am_save)\n",
    "            written += 1\n",
    "    ok(f\"Augmented pairs written: {written} → {out_img} / {out_msk}\")\n",
    "\n",
    "# UI button\n",
    "_btn_aug = Button(description=\"Run Step 3.1: Augment Dataset\", button_style=\"primary\")\n",
    "_lbl_aug = HTML()\n",
    "def _on_aug(_):\n",
    "    try:\n",
    "        run_augmentation()\n",
    "        _lbl_aug.value = \"<b>Augmentation done.</b>\"\n",
    "    except Exception as e:\n",
    "        _lbl_aug.value = f\"<span style='color:#b00'>Error: {e}</span>\"\n",
    "_btn_aug.on_click(_on_aug)\n",
    "display(VBox([_btn_aug, _lbl_aug]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d34ada",
   "metadata": {},
   "source": [
    "## Step 4: Let the model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9479aedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Fine-tune cell skipped: num_samples should be a positive integer value, but got num_samples=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DSBG-Public\\AppData\\Roaming\\Python\\Python310\\site-packages\\albumentations\\core\\validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from PIL import Image\n",
    "\n",
    "    class PairedTifDataset(Dataset):\n",
    "        def __init__(self, img_dir: Path, msk_dir: Path, aug: A.Compose|None=None):\n",
    "            self.imgs = sorted(img_dir.glob(\"*.tif\"))\n",
    "            self.msk_dir = msk_dir\n",
    "            self.aug = aug\n",
    "        def __len__(self): return len(self.imgs)\n",
    "        def __getitem__(self, i):\n",
    "            ip = self.imgs[i]\n",
    "            mp = self.msk_dir / ip.name\n",
    "            x = tiff.imread(ip).astype(np.float32)\n",
    "            y = (tiff.imread(mp) > 0).astype(np.uint8)\n",
    "            if HP.get(\"normalize\", True) and x.max() > 0:\n",
    "                x = x / x.max()\n",
    "            if self.aug is not None:\n",
    "                out = self.aug(image=x, mask=y)\n",
    "                x, y = out[\"image\"], out[\"mask\"]\n",
    "            x = torch.from_numpy(x).unsqueeze(0).repeat(3,1,1)  \n",
    "            y = torch.from_numpy(y).long()\n",
    "            return x, y\n",
    "\n",
    "    # Choose which dirs to use \n",
    "    _p = Path(in_path.value).expanduser().resolve()\n",
    "    _img_dir = _p.parent / (f\"{HP['tif_subdir']}_aug\" if (_p.parent / f\"{HP['tif_subdir']}_aug\").exists() else HP[\"tif_subdir\"])\n",
    "    _msk_dir = _p.parent / (f\"{HP['mask_subdir']}_aug\" if (_p.parent / f\"{HP['mask_subdir']}_aug\").exists() else HP[\"mask_subdir\"])\n",
    "\n",
    "    _train_aug = _build_augs(HP.get(\"aug_preset\",\"basic\")) if HP.get(\"aug_preset\",\"basic\")!=\"none\" else None\n",
    "    ds = PairedTifDataset(_img_dir, _msk_dir, _train_aug)\n",
    "    dl = DataLoader(ds, batch_size=int(HP.get(\"batch_size\",4)), shuffle=True, num_workers=2)\n",
    "\n",
    "    \n",
    "    from SegformerFinetuner import SegformerFinetuner  \n",
    "    model = SegformerFinetuner()  \n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=int(HP.get(\"max_epochs\", 50)),\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"auto\",\n",
    "        devices=1 if torch.cuda.is_available() else \"auto\",\n",
    "        precision=16 if torch.cuda.is_available() else 32,\n",
    "        log_every_n_steps=10\n",
    "    )\n",
    "    # trainer.fit(model, dl)  # Uncomment if model ready\n",
    "    info(\"Trainer prepared. Uncomment trainer.fit(...) to start training.\")\n",
    "except Exception as e:\n",
    "    warn(f\"Fine-tune cell skipped: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58a2e68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b402d3d6580438a819d1cddb2fbc274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(button_style='primary', description='Run Step 4 (SegFormer)', style=ButtonStyle()), HTML…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "def _load_hf(model_dir: str):\n",
    "    processor = None\n",
    "    # Prefer new API\n",
    "    try:\n",
    "        processor = SegformerImageProcessor.from_pretrained(model_dir)\n",
    "    except Exception:\n",
    "        processor = SegformerFeatureExtractor.from_pretrained(model_dir)\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained(model_dir)\n",
    "    model.eval()\n",
    "    return model, processor\n",
    "\n",
    "def _load_pl(ckpt_path: str):\n",
    "    # Attempt to import SegformerFinetuner from the provided notebook context\n",
    "    try:\n",
    "        from importlib import import_module\n",
    "        from SegformerFinetuner import SegformerFinetuner \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"PyTorch Lightning backend selected, but SegformerFinetuner class is not importable. \"\n",
    "                           \"Place its module on PYTHONPATH or switch to 'hf' backend.\") from e\n",
    "\n",
    "def _read_tif_batch(paths: List[Path]) -> Tuple[torch.Tensor, List[Tuple[int,int]]]:\n",
    "    imgs: List[Any] = []\n",
    "    shapes: List[Tuple[int,int]] = []\n",
    "    for p in paths:\n",
    "        arr = tiff.imread(p)  # typically a NumPy array\n",
    "        # keep float32 for the net\n",
    "        try:\n",
    "            arr = arr.astype(\"float32\", copy=False)  # if it's a NumPy array\n",
    "        except Exception:\n",
    "            pass  # not a NumPy array; leave as-is\n",
    "        shapes.append((int(arr.shape[-2]), int(arr.shape[-1])) if hasattr(arr, \"shape\") else (None, None))\n",
    "        imgs.append(arr)\n",
    "\n",
    "    # Try fast path with NumPy; otherwise fall back to pure-Python tensors\n",
    "    try:\n",
    "        import numpy as np  # ensure available name\n",
    "        ten = torch.from_numpy(np.stack(imgs, 0)).float()  # (B,H,W)\n",
    "    except Exception:\n",
    "        # Fallback: avoid torch.from_numpy by converting to lists\n",
    "        ten = torch.stack([torch.tensor(img.tolist(), dtype=torch.float32) for img in imgs], dim=0)\n",
    "\n",
    "    ten = ten.unsqueeze(1)  # (B,1,H,W)\n",
    "    return ten, shapes\n",
    "\n",
    "def _save_mask_tif(path: Path, tensor: torch.Tensor) -> None:\n",
    "    t = tensor.detach().to(\"cpu\", dtype=torch.uint8).contiguous()  # (H,W)\n",
    "    try:\n",
    "        import numpy as np  # fast path if NumPy is available\n",
    "        import tifffile as tiff\n",
    "        tiff.imwrite(path, t.numpy())\n",
    "        return\n",
    "    except Exception:\n",
    "        # NumPy unavailable → fallback to PIL from raw bytes\n",
    "        h, w = int(t.shape[-2]), int(t.shape[-1])\n",
    "        raw = bytes(t.view(-1).tolist()),
    "        img = Image.frombytes(\"L\", (w, h), raw)\n",
    "        img.save(path, format=\"TIFF\")\n",
    "\n",
    "\n",
    "def _resize_if_needed(x: torch.Tensor, size: int) -> torch.Tensor:\n",
    "    if size and size>0 and (x.shape[-1]!=size or x.shape[-2]!=size):\n",
    "        x = F.interpolate(x, size=(size,size), mode=\"bilinear\", align_corners=False)\n",
    "    return x\n",
    "\n",
    "def predict_segformer(images_dir: Path, masks_out_dir: Path) -> int:\n",
    "    masks_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    slice_paths = sorted(images_dir.glob(\"*.tif\"))\n",
    "    assert slice_paths, f\"No TIFs found in {images_dir}\"\n",
    "\n",
    "    backend = HP.get(\"inference_backend\",\"hf\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if backend == \"hf\":\n",
    "        model_dir = HP.get(\"hf_model_dir\") or \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    "        model, processor = _load_hf(model_dir)\n",
    "        model.to(device)\n",
    "    else:\n",
    "        _load_pl(HP.get(\"pl_ckpt_path\",\"\"))\n",
    "        raise RuntimeError(\"Lightning backend is stubbed. Provide a HF model dir for immediate use.\")\n",
    "\n",
    "    bs = int(HP.get(\"batch_size\",4))\n",
    "    resize_to = int(HP.get(\"resize_to\",512))\n",
    "    target_class = int(HP.get(\"target_class_idx\",1))\n",
    "    threshold = float(HP.get(\"threshold\",0.5))\n",
    "\n",
    "    written = 0\n",
    "    for i in range(0, len(slice_paths), bs):\n",
    "        batch_paths = slice_paths[i:i+bs]\n",
    "        x, orig_shapes = _read_tif_batch(batch_paths)  # (B,1,H,W)\n",
    "        x = _resize_if_needed(x, resize_to)\n",
    "        x3 = x.repeat(1,3,1,1)  # (B,3,H,W)\n",
    "\n",
    "        # Use processor to build pixel_values if available\n",
    "        inputs = {\"pixel_values\": x3.to(device)}\n",
    "        with torch.no_grad():\n",
    "            out = model(**inputs)\n",
    "            logits = out.logits  # (B,num_labels,h,w)\n",
    "\n",
    "        # Upsample logits to original size\n",
    "        up = F.interpolate(logits, size=x.shape[-2:], mode=\"bilinear\", align_corners=False)  # (B,C,H,W)\n",
    "\n",
    "        # Convert to masks\n",
    "        if up.shape[1] == 1:\n",
    "            prob = torch.sigmoid(up[:,0])\n",
    "            pred = (prob >= threshold).to(torch.uint8) * 255\n",
    "        else:\n",
    "            prob = torch.softmax(up, dim=1)[:, target_class]\n",
    "            pred = (prob >= threshold).to(torch.uint8) * 255\n",
    "\n",
    "        # Resize back to original per-slice size if resized\n",
    "        if resize_to and resize_to>0:\n",
    "            out_resized = []\n",
    "            for j in range(pred.shape[0]):\n",
    "                pj = pred[j:j+1].float().unsqueeze(0)  # (1,1,H,W)\n",
    "                pj = F.interpolate(pj, size=orig_shapes[j], mode=\"nearest\")\n",
    "                out_resized.append(pj[0,0].to(torch.uint8))\n",
    "            pred = torch.stack(out_resized, 0)\n",
    "\n",
    "        # Save\n",
    "        for pth, pmask in zip(batch_paths, pred):\n",
    "            _save_mask_tif(masks_out_dir / pth.name, pmask)\n",
    "            written += 1\n",
    "    return written\n",
    "\n",
    "# UI\n",
    "b4 = Button(description=\"Run Step 4 (SegFormer)\", button_style=\"primary\")\n",
    "l4 = HTML()\n",
    "def _b4(_):\n",
    "    try:\n",
    "        p = Path(in_path.value).expanduser().resolve()\n",
    "        images_dir = p.parent / \"tif_slices\"\n",
    "        n = predict_segformer(images_dir, p.parent / HP[\"pred_mask_subdir\"])\n",
    "        l4.value = f\"Wrote {n} mask TIFs.\"\n",
    "    except Exception as e:\n",
    "        import traceback, sys\n",
    "        err_type, err_val, err_tb = sys.exc_info()\n",
    "        tb_str = \"\".join(traceback.format_exception(err_type, err_val, err_tb))\n",
    "        print(\"==== ERROR TRACEBACK ====\")\n",
    "        print(tb_str)\n",
    "        l4.value = (\n",
    "            \"<b style='color:#b00'>Error during Step 4.</b><br>\"\n",
    "            f\"<pre style='white-space:pre-wrap'>{tb_str}</pre>\"\n",
    "        )\n",
    "b4.on_click(_b4)\n",
    "display(VBox([b4, l4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7cb2112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4cb517d791e492faaa5c19c27c8cc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(button_style='primary', description='Run Step 5', style=ButtonStyle()), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def load_tif_stack(folder: Path) -> np.ndarray:\n",
    "    files = sorted(folder.glob(\"*.tif\"))\n",
    "    assert files, f\"No .tif files in {folder}\"\n",
    "    return np.stack([tiff.imread(f).astype(np.float32) for f in files], axis=0)\n",
    "\n",
    "def save_nrrd(path: Path, array: np.ndarray, compressed: bool=True, header: Optional[dict]=None):\n",
    "    header = dict(header or {})\n",
    "    if compressed:\n",
    "        header[\"encoding\"] = \"gzip\"\n",
    "    nrrd.write(str(path), array, header=header)\n",
    "\n",
    "def do_step5():\n",
    "    p = Path(in_path.value).expanduser().resolve()\n",
    "    img_dir = p.parent / \"tif_slices\"\n",
    "    msk_dir = p.parent / HP[\"pred_mask_subdir\"]\n",
    "    imgs = load_tif_stack(img_dir)\n",
    "    msks = load_tif_stack(msk_dir)\n",
    "    _thr = float(HP.get(\"mask_bin_threshold\", 0.5))\n",
    "    if msks.dtype.kind in (\"f\",):\n",
    "        msks = (msks >= _thr).astype(np.uint8)\n",
    "    else:\n",
    "        if msks.max() > 1:\n",
    "            msks = (msks > 0).astype(np.uint8)\n",
    "    assert imgs.shape == msks.shape, f\"Shape mismatch {imgs.shape} vs {msks.shape}\"\n",
    "    out_vol = p.parent / f\"{p.stem}_volume.nrrd\"\n",
    "    out_seg = p.parent / f\"{p.stem}_segmentation.nrrd\"\n",
    "    save_nrrd(out_vol, imgs, compressed=HP[\"save_compressed_nrrd\"])\n",
    "    save_nrrd(out_seg, msks, compressed=HP[\"save_compressed_nrrd\"])\n",
    "    ok(f\"Wrote: {out_vol} and {out_seg}\")\n",
    "\n",
    "b5 = Button(description=\"Run Step 5\", button_style=\"primary\")\n",
    "l5 = HTML()\n",
    "def _b5(_):\n",
    "    try:\n",
    "        do_step5()\n",
    "        l5.value = \"Done.\"\n",
    "    except Exception as e:\n",
    "        l5.value = f\"<span style='color:#b00'>Error: {e}</span>\"\n",
    "b5.on_click(_b5)\n",
    "display(VBox([b5, l5]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
