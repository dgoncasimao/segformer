{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2d21c2",
   "metadata": {},
   "source": [
    "\n",
    "# Model to mask Vastus Lateralis\n",
    "\n",
    "1) Provide input video file \n",
    "2) Let the model run \n",
    "3) Once it has finished the predicted video will be in the same folder as the input \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95701770",
   "metadata": {},
   "source": [
    "\n",
    "## How to Use \n",
    "1. **Install dependencies** run the next cell    \n",
    "2. **Set hyperparameters** choose to keep defaults or change the parameters.  \n",
    "3. **Provide your input video file** insert the path to your video. \n",
    "4. **Run the rest of the cells** click on \"execute cell and below\"\n",
    "5. **Outputs are saved alongside the input file**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381d9259",
   "metadata": {},
   "source": [
    "## Step 1: Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d31dc588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement nrrd (from versions: none)\n",
      "ERROR: No matching distribution found for nrrd\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Install libs (run once)\n",
    "%pip install -q numpy tifffile nrrd ipywidgets torch torchvision torchaudio transformers pytorch_lightning pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8f2ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n",
      "c:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "import nrrd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor, SegformerFeatureExtractor\n",
    "\n",
    "from ipywidgets import VBox, HBox, Text, Button, Checkbox, Dropdown, IntText, FloatText, HTML, Accordion, Layout\n",
    "from IPython.display import display\n",
    "\n",
    "def info(msg): print(f\"[INFO] {msg}\")\n",
    "def warn(msg): print(f\"[WARN] {msg}\")\n",
    "def ok(msg):   print(f\"[OK]   {msg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca794f0e",
   "metadata": {},
   "source": [
    "## Step 2: Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d98ffde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484ce8e6ab254fa5a79c995110e716ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HBox(children=(Dropdown(description='backend', options=('hf', 'pl'), value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "HP: Dict[str, Any] = {\n",
    "    # Converter + IO\n",
    "    \"use_internal_nrrd_to_tif\": True,\n",
    "    \"tif_subdir\": \"tif_slices\",\n",
    "    \"mask_subdir\": \"pred_masks\",\n",
    "    \"overwrite_outputs\": True,\n",
    "    \"save_compressed_nrrd\": True,\n",
    "    \"make_single_nrrd\": True,\n",
    "    \"also_save_separate\": False,\n",
    "\n",
    "    # Model\n",
    "    \"inference_backend\": \"hf\",   # \"hf\" (Hugging Face) or \"pl\" (PyTorch Lightning)\n",
    "    \"hf_model_dir\": \"\",          # e.g., a local dir with config+weights OR a HF repo id\n",
    "    \"pl_ckpt_path\": \"\",          # e.g., /path/to/lightning/checkpoints/best.ckpt\n",
    "    \"target_class_idx\": 1,       # used if model has multiple classes; this is the positive class to extract\n",
    "    \"threshold\": 0.5,            # used if model produces single-channel logits (sigmoid) or after softmax prob for target_class\n",
    "    \"batch_size\": 4,\n",
    "    \"resize_to\": 512,            # if >0, resize (H,W) to this square size for model; output is resized back to original\n",
    "\n",
    "    # Preprocessing\n",
    "    \"normalize\": True,           # simple 0-1 normalization\n",
    "}\n",
    "# Quick widget\n",
    "w_backend  = Dropdown(options=[\"hf\",\"pl\"], value=HP[\"inference_backend\"], description=\"backend\")\n",
    "w_hf_dir   = Text(value=HP[\"hf_model_dir\"], description=\"hf_model_dir\", layout=Layout(width=\"60%\"))\n",
    "w_pl_ckpt  = Text(value=HP[\"pl_ckpt_path\"], description=\"pl_ckpt_path\", layout=Layout(width=\"60%\"))\n",
    "w_tclass   = IntText(value=HP[\"target_class_idx\"], description=\"target_class_idx\")\n",
    "w_thr      = FloatText(value=HP[\"threshold\"], description=\"threshold\")\n",
    "w_bs       = IntText(value=HP[\"batch_size\"], description=\"batch_size\")\n",
    "w_resize   = IntText(value=HP[\"resize_to\"], description=\"resize_to\")\n",
    "w_overwr   = Checkbox(value=HP[\"overwrite_outputs\"], description=\"overwrite_outputs\")\n",
    "w_single   = Checkbox(value=HP[\"make_single_nrrd\"], description=\"make_single_nrrd\")\n",
    "w_sep      = Checkbox(value=HP[\"also_save_separate\"], description=\"also_save_separate\")\n",
    "\n",
    "def _apply(_):\n",
    "    HP.update({\n",
    "        \"inference_backend\": w_backend.value,\n",
    "        \"hf_model_dir\": w_hf_dir.value.strip(),\n",
    "        \"pl_ckpt_path\": w_pl_ckpt.value.strip(),\n",
    "        \"target_class_idx\": int(w_tclass.value),\n",
    "        \"threshold\": float(w_thr.value),\n",
    "        \"batch_size\": int(w_bs.value),\n",
    "        \"resize_to\": int(w_resize.value),\n",
    "        \"overwrite_outputs\": w_overwr.value,\n",
    "        \"make_single_nrrd\": w_single.value,\n",
    "        \"also_save_separate\": w_sep.value,\n",
    "        \"max_epochs\": 50,          \n",
    "        \"aug_preset\": \"basic\",     \n",
    "        \"aug_count\": 2,            \n",
    "        \"aug_seed\": 42,            \n",
    "    })\n",
    "    ok(\"Hyperparameters updated.\")\n",
    "btn = Button(description=\"Apply\", button_style=\"primary\")\n",
    "btn.on_click(_apply)\n",
    "display(Accordion(children=[\n",
    "    VBox([\n",
    "        HBox([w_backend, w_bs, w_resize]),\n",
    "        HBox([w_tclass, w_thr]),\n",
    "        w_hf_dir, w_pl_ckpt,\n",
    "        HBox([w_overwr, w_single, w_sep]),\n",
    "        btn\n",
    "    ])\n",
    "], titles=(\"Hyperparameters\",)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d783452",
   "metadata": {},
   "source": [
    "## Step 3: Input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ee0848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffdf6cf604254176a39e5dd4b080427e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='', description='Input .nrrd', layout=Layout(width='80%'), placeholde…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_path = Text(description=\"Input .nrrd\", placeholder=\"/path/to/volume.nrrd\", layout=Layout(width=\"80%\"))\n",
    "btn_val = Button(description=\"Validate Path\", button_style=\"primary\")\n",
    "lbl_val = HTML()\n",
    "def _val(_):\n",
    "    p = Path(in_path.value).expanduser().resolve()\n",
    "    lbl_val.value = f\"<b style='color:green'>OK</b>: {p}\" if p.exists() and p.suffix.lower()==\".nrrd\" else f\"<b style='color:#b00'>Invalid</b>: {p}\"\n",
    "btn_val.on_click(_val)\n",
    "display(VBox([HBox([in_path, btn_val]), lbl_val]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63b26ea",
   "metadata": {},
   "source": [
    "# Step 3.1: Dataset Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9c4968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007a5b552c7f4e2a86fb9b9d60abc6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(button_style='primary', description='Run Step 3.1: Augment Dataset', style=ButtonStyle()…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "def _build_augs(preset: str) -> A.Compose:\n",
    "    if preset == \"none\":\n",
    "        return A.Compose([], is_check_shapes=False)\n",
    "    if preset == \"strong\":\n",
    "        return A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.2),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.06, scale_limit=0.15, rotate_limit=25, border_mode=0, p=0.7),\n",
    "            A.ElasticTransform(alpha=20, sigma=4, alpha_affine=10, border_mode=0, p=0.3),\n",
    "            A.GaussNoise(var_limit=(5.0, 20.0), p=0.3),\n",
    "            A.RandomBrightnessContrast(p=0.4),\n",
    "        ], is_check_shapes=False)\n",
    "    # basic\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.3),\n",
    "        A.ShiftScaleRotate(shift_limit=0.03, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.25),\n",
    "    ], is_check_shapes=False)\n",
    "\n",
    "def run_augmentation():\n",
    "    p = Path(in_path.value).expanduser().resolve()\n",
    "    img_dir = p.parent / HP[\"tif_subdir\"]       \n",
    "    msk_dir = p.parent / HP[\"mask_subdir\"]      \n",
    "    assert img_dir.exists() and msk_dir.exists(), \"Run Step 3 first to create paired TIFs.\"\n",
    "\n",
    "    out_img = p.parent / f\"{HP['tif_subdir']}_aug\"\n",
    "    out_msk = p.parent / f\"{HP['mask_subdir']}_aug\"\n",
    "    out_img.mkdir(parents=True, exist_ok=True)\n",
    "    out_msk.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    preset = HP.get(\"aug_preset\", \"basic\")\n",
    "    copies = int(HP.get(\"aug_count\", 2))\n",
    "    seed = int(HP.get(\"aug_seed\", 42))\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    t = _build_augs(preset)\n",
    "    pairs = sorted(img_dir.glob(\"frame_*.tif\"))\n",
    "    assert pairs, f\"No frames found in {img_dir}\"\n",
    "\n",
    "    written = 0\n",
    "    for img_path in pairs:\n",
    "        m_path = (msk_dir / img_path.name)\n",
    "        if not m_path.exists():\n",
    "            continue\n",
    "        img = tiff.imread(img_path)\n",
    "        msk = tiff.imread(m_path)\n",
    "\n",
    "        # ensure 2D\n",
    "        if img.ndim == 3:\n",
    "            img = img.squeeze()\n",
    "        if msk.ndim == 3:\n",
    "            msk = msk.squeeze()\n",
    "\n",
    "        for k in range(copies):\n",
    "            aug = t(image=img, mask=msk)\n",
    "            ai, am = aug[\"image\"], aug[\"mask\"]\n",
    "\n",
    "            # keep dtype-friendly saves\n",
    "            ai_save = ai.astype(np.uint16 if ai.max() > 255 else np.uint8)\n",
    "            am_save = (am > 0).astype(np.uint8) * 255\n",
    "\n",
    "            stem = img_path.stem  # frame_00000\n",
    "            tiff.imwrite(out_img / f\"{stem}_aug{k:02d}.tif\", ai_save)\n",
    "            tiff.imwrite(out_msk / f\"{stem}_aug{k:02d}.tif\", am_save)\n",
    "            written += 1\n",
    "    ok(f\"Augmented pairs written: {written} → {out_img} / {out_msk}\")\n",
    "\n",
    "# UI button\n",
    "_btn_aug = Button(description=\"Run Step 3.1: Augment Dataset\", button_style=\"primary\")\n",
    "_lbl_aug = HTML()\n",
    "def _on_aug(_):\n",
    "    try:\n",
    "        run_augmentation()\n",
    "        _lbl_aug.value = \"<b>Augmentation done.</b>\"\n",
    "    except Exception as e:\n",
    "        _lbl_aug.value = f\"<span style='color:#b00'>Error: {e}</span>\"\n",
    "_btn_aug.on_click(_on_aug)\n",
    "display(VBox([_btn_aug, _lbl_aug]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d34ada",
   "metadata": {},
   "source": [
    "## Step 4: Let the model run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1903f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8062f23f00ec4098ac80729116892975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(button_style='primary', description='Run Step 3 (masked+cropped)', style=ButtonStyle()),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def crop_image_get_bounds(image, threshold):\n",
    "    rows = np.any(image > threshold, axis=1)\n",
    "    cols = np.any(image > threshold, axis=0)\n",
    "    if not np.any(rows) or not np.any(cols):\n",
    "        return None\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    return rmin, rmax, cmin, cmax\n",
    "\n",
    "def apply_crop(image, bounds):\n",
    "    if bounds is None:\n",
    "        return None\n",
    "    rmin, rmax, cmin, cmax = bounds\n",
    "    return image[rmin:rmax+1, cmin:cmax+1]\n",
    "\n",
    "def resize_with_padding(image, target_size=(512, 512)):\n",
    "    old_h, old_w = image.shape[:2]\n",
    "    tgt_h, tgt_w = target_size\n",
    "    delta_w = max(tgt_w - old_w, 0)\n",
    "    delta_h = max(tgt_h - old_h, 0)\n",
    "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "    return np.pad(image, ((top, bottom), (left, right)), mode='constant', constant_values=0)\n",
    "\n",
    "def export_to_tif_with_threshold(volume_data, mask_data, out_vol_dir, out_mask_dir, threshold=10, pad_size=1500):\n",
    "    exported_count = 0\n",
    "    if volume_data.shape[0] != mask_data.shape[0]:\n",
    "        # Try to detect if the data is (Y,X,Z)\n",
    "        if volume_data.ndim == 3 and volume_data.shape[2] == mask_data.shape[2]:\n",
    "            volume_data = np.transpose(volume_data, (2,0,1))\n",
    "            mask_data = np.transpose(mask_data, (2,0,1))\n",
    "    assert volume_data.shape == mask_data.shape, f\"Volume/mask shapes differ: {volume_data.shape} vs {mask_data.shape}\"\n",
    "    Z = volume_data.shape[0]\n",
    "\n",
    "    out_vol_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_mask_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i in range(Z):\n",
    "        slice_volume = volume_data[i]\n",
    "        slice_mask = mask_data[i]\n",
    "\n",
    "        masked_volume = slice_volume * slice_mask\n",
    "        if not np.any(masked_volume > threshold):\n",
    "            continue\n",
    "\n",
    "        bounds = crop_image_get_bounds(slice_volume, threshold)\n",
    "        cropped_volume = apply_crop(slice_volume, bounds)\n",
    "        cropped_mask = apply_crop(slice_mask, bounds)\n",
    "        if cropped_volume is None or cropped_mask is None:\n",
    "            continue\n",
    "\n",
    "        volume_resized = resize_with_padding(cropped_volume, target_size=(pad_size, pad_size))\n",
    "        mask_resized = resize_with_padding(cropped_mask, target_size=(pad_size, pad_size))\n",
    "\n",
    "        vpath = out_vol_dir / f\"frame_{exported_count:05d}.tif\"\n",
    "        mpath = out_mask_dir / f\"frame_{exported_count:05d}.tif\"\n",
    "        Image.fromarray(volume_resized.astype(np.uint16 if volume_resized.max()>255 else np.uint8)).save(vpath)\n",
    "        Image.fromarray((mask_resized>0).astype(np.uint8)*255).save(mpath)\n",
    "        exported_count += 1\n",
    "    return exported_count\n",
    "\n",
    "def _infer_volume_and_mask(vol):\n",
    "    # Return (vol3d, mask3d).\n",
    "    if vol.ndim == 4 and vol.shape[0] == 2:\n",
    "        img = vol[0]\n",
    "        msk = vol[1]\n",
    "        return img, msk\n",
    "    if vol.ndim == 3:\n",
    "        return vol, None\n",
    "    raise ValueError(f\"Unsupported NRRD shape: {vol.shape}\")\n",
    "\n",
    "def internal_nrrd_to_tifs_masked(nrrd_path: Path, out_dir_images: Path, out_dir_masks: Path) -> int:\n",
    "    data, header = nrrd.read(str(nrrd_path))\n",
    "    data = np.asarray(data)\n",
    "    img, msk = _infer_volume_and_mask(data)\n",
    "    if msk is None:\n",
    "        # try sibling masks\n",
    "        p = nrrd_path\n",
    "        cands = [p.with_name(p.stem + \"_masks.nrrd\"),\n",
    "                 p.with_name(p.stem + \"_mask.nrrd\")]\n",
    "        for c in cands:\n",
    "            if c.exists():\n",
    "                msk, _ = nrrd.read(str(c))\n",
    "                break\n",
    "    if msk is None and HP.get(\"strict_require_mask\", False):\n",
    "        raise RuntimeError(\"No mask volume provided/found and strict_require_mask=True\")\n",
    "    if msk is None:\n",
    "        # Fallback: create full-ones mask \n",
    "        msk = np.ones_like(img)\n",
    "\n",
    "    # Ensure shapes are (Z,Y,X)\n",
    "    if img.ndim == 3 and img.shape[0] != msk.shape[0]:\n",
    "        if img.shape[2] == msk.shape[2]:\n",
    "            img = np.transpose(img, (2,0,1))\n",
    "            msk = np.transpose(msk, (2,0,1))\n",
    "\n",
    "    n = export_to_tif_with_threshold(\n",
    "        img, msk, out_dir_images, out_dir_masks,\n",
    "        threshold=int(HP.get(\"crop_threshold\", 10)),\n",
    "        pad_size=int(HP.get(\"pad_size\", 1500))\n",
    "    )\n",
    "    return n\n",
    "\n",
    "def do_step3():\n",
    "    p = Path(in_path.value).expanduser().resolve()\n",
    "    assert p.exists() and p.suffix.lower()==\".nrrd\", \"Set a valid .nrrd path.\"\n",
    "    out_root = p.parent\n",
    "    tif_dir  = out_root / HP[\"tif_subdir\"]\n",
    "    gt_dir   = out_root / HP[\"mask_subdir\"]\n",
    "    if not HP[\"overwrite_outputs\"] and (any(tif_dir.glob(\"*.tif\")) or any(gt_dir.glob(\"*.tif\"))):\n",
    "        warn(\"Output dirs not empty; skipping due to overwrite_outputs=False\")\n",
    "        return tif_dir\n",
    "    n = internal_nrrd_to_tifs_masked(p, tif_dir, gt_dir)\n",
    "    ok(f\"Wrote {n} TIF pairs to {tif_dir} and {gt_dir}\")\n",
    "    return tif_dir\n",
    "\n",
    "b3 = Button(description=\"Run Step 3 (masked+cropped)\", button_style=\"primary\")\n",
    "l3 = HTML()\n",
    "def _b3(_):\n",
    "    try:\n",
    "        out = do_step3()\n",
    "        l3.value = f\"Done: <code>{out}</code>\"\n",
    "    except Exception as e:\n",
    "        l3.value = f\"<span style='color:#b00'>Error: {e}</span>\"\n",
    "b3.on_click(_b3)\n",
    "display(VBox([b3, l3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9479aedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Fine-tune cell skipped: num_samples should be a positive integer value, but got num_samples=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\albumentations\\core\\validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# ## Optional — Fine-tune SegFormer with max_epochs\n",
    "# Requires your Lightning module (e.g., SegformerFinetuner) to be importable.\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    from PIL import Image\n",
    "\n",
    "    class PairedTifDataset(Dataset):\n",
    "        def __init__(self, img_dir: Path, msk_dir: Path, aug: A.Compose|None=None):\n",
    "            self.imgs = sorted(img_dir.glob(\"*.tif\"))\n",
    "            self.msk_dir = msk_dir\n",
    "            self.aug = aug\n",
    "        def __len__(self): return len(self.imgs)\n",
    "        def __getitem__(self, i):\n",
    "            ip = self.imgs[i]\n",
    "            mp = self.msk_dir / ip.name\n",
    "            x = tiff.imread(ip).astype(np.float32)\n",
    "            y = (tiff.imread(mp) > 0).astype(np.uint8)\n",
    "            if HP.get(\"normalize\", True) and x.max() > 0:\n",
    "                x = x / x.max()\n",
    "            if self.aug is not None:\n",
    "                out = self.aug(image=x, mask=y)\n",
    "                x, y = out[\"image\"], out[\"mask\"]\n",
    "            x = torch.from_numpy(x).unsqueeze(0).repeat(3,1,1)  \n",
    "            y = torch.from_numpy(y).long()\n",
    "            return x, y\n",
    "\n",
    "    # Choose which dirs to use \n",
    "    _p = Path(in_path.value).expanduser().resolve()\n",
    "    _img_dir = _p.parent / (f\"{HP['tif_subdir']}_aug\" if (_p.parent / f\"{HP['tif_subdir']}_aug\").exists() else HP[\"tif_subdir\"])\n",
    "    _msk_dir = _p.parent / (f\"{HP['mask_subdir']}_aug\" if (_p.parent / f\"{HP['mask_subdir']}_aug\").exists() else HP[\"mask_subdir\"])\n",
    "\n",
    "    _train_aug = _build_augs(HP.get(\"aug_preset\",\"basic\")) if HP.get(\"aug_preset\",\"basic\")!=\"none\" else None\n",
    "    ds = PairedTifDataset(_img_dir, _msk_dir, _train_aug)\n",
    "    dl = DataLoader(ds, batch_size=int(HP.get(\"batch_size\",4)), shuffle=True, num_workers=2)\n",
    "\n",
    "    \n",
    "    from SegformerFinetuner import SegformerFinetuner  \n",
    "    model = SegformerFinetuner()  \n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=int(HP.get(\"max_epochs\", 50)),\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"auto\",\n",
    "        devices=1 if torch.cuda.is_available() else \"auto\",\n",
    "        precision=16 if torch.cuda.is_available() else 32,\n",
    "        log_every_n_steps=10\n",
    "    )\n",
    "    # trainer.fit(model, dl)  # Uncomment if model ready\n",
    "    info(\"Trainer prepared. Uncomment trainer.fit(...) to start training.\")\n",
    "except Exception as e:\n",
    "    warn(f\"Fine-tune cell skipped: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a2e68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a14d74b72cb4a6f8e30d174edb25e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(button_style='primary', description='Run Step 4 (SegFormer)', style=ButtonStyle()), HTML…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def _load_hf(model_dir: str):\n",
    "    processor = None\n",
    "    # Prefer new API\n",
    "    try:\n",
    "        processor = SegformerImageProcessor.from_pretrained(model_dir)\n",
    "    except Exception:\n",
    "        processor = SegformerFeatureExtractor.from_pretrained(model_dir)\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained(model_dir)\n",
    "    model.eval()\n",
    "    return model, processor\n",
    "\n",
    "def _load_pl(ckpt_path: str):\n",
    "    # Attempt to import SegformerFinetuner from the provided notebook context\n",
    "    try:\n",
    "        from importlib import import_module\n",
    "        from SegformerFinetuner import SegformerFinetuner \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"PyTorch Lightning backend selected, but SegformerFinetuner class is not importable. \"\n",
    "                           \"Place its module on PYTHONPATH or switch to 'hf' backend.\") from e\n",
    "\n",
    "def _read_tif_batch(paths: List[Path]) -> Tuple[torch.Tensor, List[Tuple[int,int]]]:\n",
    "    imgs = []\n",
    "    shapes = []\n",
    "    for p in paths:\n",
    "        arr = tiff.imread(p).astype(np.float32)\n",
    "        shapes.append(arr.shape)  # (H,W)\n",
    "        if HP[\"normalize\"] and arr.max()>0:\n",
    "            arr = arr / arr.max()\n",
    "        imgs.append(arr)\n",
    "    ten = torch.from_numpy(np.stack(imgs, 0))  # (B,H,W)\n",
    "    ten = ten.unsqueeze(1)  # (B,1,H,W)\n",
    "    return ten, shapes\n",
    "\n",
    "def _resize_if_needed(x: torch.Tensor, size: int) -> torch.Tensor:\n",
    "    if size and size>0 and (x.shape[-1]!=size or x.shape[-2]!=size):\n",
    "        x = F.interpolate(x, size=(size,size), mode=\"bilinear\", align_corners=False)\n",
    "    return x\n",
    "\n",
    "def predict_segformer(images_dir: Path, masks_out_dir: Path) -> int:\n",
    "    masks_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    slice_paths = sorted(images_dir.glob(\"*.tif\"))\n",
    "    assert slice_paths, f\"No TIFs found in {images_dir}\"\n",
    "\n",
    "    backend = HP.get(\"inference_backend\",\"hf\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if backend == \"hf\":\n",
    "        model_dir = HP.get(\"hf_model_dir\") or \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    "        model, processor = _load_hf(model_dir)\n",
    "        model.to(device)\n",
    "    else:\n",
    "        _load_pl(HP.get(\"pl_ckpt_path\",\"\"))\n",
    "        raise RuntimeError(\"Lightning backend is stubbed. Provide a HF model dir for immediate use.\")\n",
    "\n",
    "    bs = int(HP.get(\"batch_size\",4))\n",
    "    resize_to = int(HP.get(\"resize_to\",512))\n",
    "    target_class = int(HP.get(\"target_class_idx\",1))\n",
    "    threshold = float(HP.get(\"threshold\",0.5))\n",
    "\n",
    "    written = 0\n",
    "    for i in range(0, len(slice_paths), bs):\n",
    "        batch_paths = slice_paths[i:i+bs]\n",
    "        x, orig_shapes = _read_tif_batch(batch_paths)  # (B,1,H,W)\n",
    "        x = _resize_if_needed(x, resize_to)\n",
    "        x3 = x.repeat(1,3,1,1)  # (B,3,H,W)\n",
    "\n",
    "        # Use processor to build pixel_values if available\n",
    "        inputs = {\"pixel_values\": x3.to(device)}\n",
    "        with torch.no_grad():\n",
    "            out = model(**inputs)\n",
    "            logits = out.logits  # (B,num_labels,h,w)\n",
    "\n",
    "        # Upsample logits to original (or current) size\n",
    "        up = F.interpolate(logits, size=x.shape[-2:], mode=\"bilinear\", align_corners=False)  # (B,C,H,W)\n",
    "\n",
    "        # Convert to masks\n",
    "        if up.shape[1] == 1:\n",
    "            prob = torch.sigmoid(up[:,0])\n",
    "            pred = (prob >= threshold).to(torch.uint8) * 255\n",
    "        else:\n",
    "            prob = torch.softmax(up, dim=1)[:, target_class]\n",
    "            pred = (prob >= threshold).to(torch.uint8) * 255\n",
    "\n",
    "        # Resize back to original per-slice size if we resized\n",
    "        if resize_to and resize_to>0:\n",
    "            out_resized = []\n",
    "            for j in range(pred.shape[0]):\n",
    "                pj = pred[j:j+1].float().unsqueeze(0)  # (1,1,H,W)\n",
    "                pj = F.interpolate(pj, size=orig_shapes[j], mode=\"nearest\")\n",
    "                out_resized.append(pj[0,0].to(torch.uint8))\n",
    "            pred = torch.stack(out_resized, 0)\n",
    "\n",
    "        # Save\n",
    "        for pth, pmask in zip(batch_paths, pred):\n",
    "            tiff.imwrite(masks_out_dir / pth.name, pmask.cpu().numpy())\n",
    "            written += 1\n",
    "    return written\n",
    "\n",
    "# UI\n",
    "b4 = Button(description=\"Run Step 4 (SegFormer)\", button_style=\"primary\")\n",
    "l4 = HTML()\n",
    "def _b4(_):\n",
    "    try:\n",
    "        p = Path(in_path.value).expanduser().resolve()\n",
    "        images_dir = p.parent / \"tif_slices\"\n",
    "        n = predict_segformer(images_dir, p.parent / HP[\"mask_subdir\"])\n",
    "        l4.value = f\"Wrote {n} mask TIFs.\"\n",
    "    except Exception as e:\n",
    "        l4.value = f\"<span style='color:#b00'>Error: {e}</span>\"\n",
    "b4.on_click(_b4)\n",
    "display(VBox([b4, l4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7cb2112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7611b9e803874b3fa3bb340af175bc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(button_style='primary', description='Run Step 5', style=ButtonStyle()), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def load_tif_stack(folder: Path) -> np.ndarray:\n",
    "    files = sorted(folder.glob(\"*.tif\"))\n",
    "    assert files, f\"No .tif files in {folder}\"\n",
    "    return np.stack([tiff.imread(f).astype(np.float32) for f in files], axis=0)\n",
    "\n",
    "def save_nrrd(path: Path, array: np.ndarray, compressed: bool=True, header: Optional[dict]=None):\n",
    "    header = dict(header or {})\n",
    "    if compressed:\n",
    "        header[\"encoding\"] = \"gzip\"\n",
    "    nrrd.write(str(path), array, header=header)\n",
    "\n",
    "def do_step5():\n",
    "    p = Path(in_path.value).expanduser().resolve()\n",
    "    img_dir = p.parent / \"tif_slices\"\n",
    "    msk_dir = p.parent / HP[\"mask_subdir\"]\n",
    "    imgs = load_tif_stack(img_dir)\n",
    "    msks = load_tif_stack(msk_dir)\n",
    "    assert imgs.shape == msks.shape, f\"Shape mismatch {imgs.shape} vs {msks.shape}\"\n",
    "    if HP[\"make_single_nrrd\"]:\n",
    "        comb = np.stack([imgs, msks], axis=0)\n",
    "        out_single = p.parent / f\"{p.stem}_imgmask.nrrd\"\n",
    "        if out_single.exists() and not HP[\"overwrite_outputs\"]:\n",
    "            warn(f\"Exists, not overwriting: {out_single}\")\n",
    "        else:\n",
    "            save_nrrd(out_single, comb, compressed=HP[\"save_compressed_nrrd\"],\n",
    "                      header={\"kinds\": [\"list\",\"domain\",\"domain\",\"domain\"]})\n",
    "            ok(f\"Wrote: {out_single}\")\n",
    "    if HP[\"also_save_separate\"]:\n",
    "        out_i = p.parent / f\"{p.stem}_images.nrrd\"\n",
    "        out_m = p.parent / f\"{p.stem}_masks.nrrd\"\n",
    "        save_nrrd(out_i, imgs, compressed=HP[\"save_compressed_nrrd\"])\n",
    "        save_nrrd(out_m, msks, compressed=HP[\"save_compressed_nrrd\"])\n",
    "        ok(f\"Wrote: {out_i} and {out_m}\")\n",
    "\n",
    "b5 = Button(description=\"Run Step 5\", button_style=\"primary\")\n",
    "l5 = HTML()\n",
    "def _b5(_):\n",
    "    try:\n",
    "        do_step5()\n",
    "        l5.value = \"Done.\"\n",
    "    except Exception as e:\n",
    "        l5.value = f\"<span style='color:#b00'>Error: {e}</span>\"\n",
    "b5.on_click(_b5)\n",
    "display(VBox([b5, l5]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Seg-Rob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
