{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "c:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch as ns\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import evaluate\n",
    "import pytorch_lightning as pl\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor, get_cosine_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import SegformerFeatureExtractor\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "import multiprocessing\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import AdamW \n",
    "import torch.optim as optim\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdiego-gonca-simao\u001b[0m (\u001b[33mdiego-gonca-simao-basel\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Configurations ===\n",
    "TRAIN_IMAGE_DIR = 'C:/Users/DSBG-Public/segformer/dataset/train/images'\n",
    "TRAIN_MASK_DIR = 'C:/Users/DSBG-Public/segformer/dataset/train/masks'\n",
    "VAL_IMAGE_DIR = 'C:/Users/DSBG-Public/segformer/dataset/val/images'\n",
    "VAL_MASK_DIR = 'C:/Users/DSBG-Public/segformer/dataset/val/masks'\n",
    "TEST_IMAGE_DIR = 'C:/Users/DSBG-Public/segformer/dataset/test/images'\n",
    "TEST_MASK_DIR = 'C:/Users/DSBG-Public/segformer/dataset/test/masks'\n",
    "NUM_CLASSES = 2\n",
    "IMG_SIZE= (512, 512)\n",
    "\n",
    "\n",
    "# === Dataset ===\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, feature_extractor, augment=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.augment = augment\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.images = sorted(os.listdir(image_dir))\n",
    "        self.masks = sorted(os.listdir(mask_dir))\n",
    "\n",
    "        if augment:\n",
    "            self.transforms = A.Compose([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.RandomRotate90(p=0.5),\n",
    "                A.RandomScale(scale_limit=(0.9, 1.1), p=0.5),\n",
    "                A.ShiftScaleRotate(shift_limit=0.05, rotate_limit=15, scale_limit=0.0, border_mode=0, p=0.5),\n",
    "\n",
    "            A.OneOf([\n",
    "                A.MotionBlur(p=0.2),\n",
    "                A.MedianBlur(blur_limit=3, p=0.1),\n",
    "                A.GaussianBlur(blur_limit=3, p=0.1),\n",
    "            ], p=0.3),\n",
    "\n",
    "            A.ColorJitter(0.3, 0.3, 0.3, 0.1, p=0.7),\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
    "\n",
    "            A.PadIfNeeded(IMG_SIZE[0], IMG_SIZE[1]),\n",
    "            A.RandomCrop(IMG_SIZE[0], IMG_SIZE[1]),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array(Image.open(os.path.join(self.image_dir, self.images[idx])).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(os.path.join(self.mask_dir, self.masks[idx])).convert(\"L\"))\n",
    "        if self.augment:\n",
    "            aug  = self.transforms(image=image, mask=mask)\n",
    "            image, mask = aug[\"image\"], aug[\"mask\"]\n",
    "        \n",
    "        encoding = self.feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        pixel_values = encoding['pixel_values'].squeeze(0)\n",
    "        \n",
    "        mask = np.array(mask)\n",
    "        mask = (mask > 127).astype(np.uint8)\n",
    "        mask = np.clip(mask, 0, NUM_CLASSES - 1)\n",
    "        mask = ns.tensor(mask, dtype=ns.long)\n",
    "\n",
    "    \n",
    "        encoding['labels'] = mask\n",
    "        encoding['pixel_values'] = pixel_values\n",
    "    \n",
    "\n",
    "        return encoding\n",
    "    \n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def forward(self, logits, targets, eps=1e-6):\n",
    "        probs = logits.softmax(1)\n",
    "        targets_oh = nn.functional.one_hot(targets, num_classes=NUM_CLASSES).permute(0,3,1,2)\n",
    "        inter = (probs*targets_oh).sum((2,3))\n",
    "        union = probs.sum((2,3)) + targets_oh.sum((2,3))\n",
    "        dice = 1 - (2*inter + eps)/(union + eps)\n",
    "        return dice.mean()\n",
    "\n",
    "dice_loss = DiceLoss()\n",
    "ce_loss   = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# === PyTorch Lightning Module ===\n",
    "class SegformerFinetuner(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, train_dataloader=None, val_dataloader=None, test_dataloader=None, metrics_interval=50):\n",
    "        super(SegformerFinetuner, self).__init__()\n",
    "        self.metrics_interval = metrics_interval\n",
    "        self.train_dl = train_dataloader\n",
    "        self.val_dl = val_dataloader\n",
    "        self.test_dl = test_dataloader\n",
    "\n",
    "        self.num_classes = NUM_CLASSES \n",
    "\n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            \"nvidia/segformer-b0-finetuned-ade-512-512\", \n",
    "            return_dict=False, \n",
    "            num_labels=self.num_classes,\n",
    "            ignore_mismatched_sizes=True,\n",
    "        )\n",
    "\n",
    "        self.train_mean_iou = evaluate.load(\"mean_iou\")\n",
    "        self.val_mean_iou = evaluate.load(\"mean_iou\")\n",
    "        self.test_mean_iou = evaluate.load(\"mean_iou\")\n",
    "\n",
    "\n",
    "    def forward(self, images, masks):\n",
    "        return self.model(pixel_values=images, labels=masks)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        labels = batch[\"labels\"]\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        outputs = self.model(pixel_values, labels)\n",
    "        loss_ce, logits = outputs[:2]\n",
    "        \n",
    "        \n",
    "        # Resize labels to match logits spatial dimensions if necessary.\n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits, \n",
    "            size=labels.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        loss = 0.4*loss_ce + 0.6*dice_loss(upsampled_logits, labels)\n",
    "\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "        self.train_mean_iou.add_batch(\n",
    "            predictions=predicted.detach().cpu().numpy(), \n",
    "            references=labels.detach().cpu().numpy()\n",
    "        )\n",
    "        if batch_idx % self.metrics_interval == 0:\n",
    "\n",
    "            metrics = self.train_mean_iou.compute(\n",
    "                num_labels=self.num_classes, \n",
    "                ignore_index=255, \n",
    "                reduce_labels=False,\n",
    "            )\n",
    "            \n",
    "            metrics = {'loss': loss, \"mean_iou\": metrics[\"mean_iou\"], \"mean_accuracy\": metrics[\"mean_accuracy\"]}\n",
    "            self.log(\"train_loss\",    loss,    prog_bar=True)         \n",
    "            self.log(\"train_mIoU\",    metrics[\"mean_iou\"], prog_bar=True)\n",
    "            self.log(\"train_acc\",     metrics[\"mean_accuracy\"], prog_bar=True)\n",
    "            for k,v in metrics.items():\n",
    "                self.log(k,v)\n",
    "            \n",
    "            return(metrics)\n",
    "        else:\n",
    "            return({'loss': loss})\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        labels = batch[\"labels\"]\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        outputs = self.model(pixel_values,labels)\n",
    "        loss_ce, logits = outputs[:2]\n",
    "        \n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits, \n",
    "            size=labels.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        loss = 0.4*loss_ce + 0.6*dice_loss(upsampled_logits, labels)\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        \n",
    "        self.val_mean_iou.add_batch(\n",
    "            predictions=predicted.detach().cpu().numpy(), \n",
    "            references=labels.detach().cpu().numpy()\n",
    "        )\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return({'val_loss': loss})\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        images, masks = batch['pixel_values'], batch['labels']\n",
    "        \n",
    "        outputs = self(images, masks)\n",
    "        \n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        \n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits, \n",
    "            size=masks.shape[-2:], \n",
    "            mode=\"bilinear\", \n",
    "            align_corners=False\n",
    "        )\n",
    "        \n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        \n",
    "        self.test_mean_iou.add_batch(\n",
    "            predictions=predicted.detach().cpu().numpy(), \n",
    "            references=masks.detach().cpu().numpy()\n",
    "        )\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return({'test_loss': loss})\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "       optimizer = optim.AdamW(self.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "       steps_per_epoch = len(self.train_dl)\n",
    "       max_iters = self.trainer.max_epochs * steps_per_epoch\n",
    "       scheduler = get_cosine_schedule_with_warmup(\n",
    "           optimizer,\n",
    "           num_warmup_steps=20,\n",
    "           num_training_steps=max_iters\n",
    "       )\n",
    "       return {\n",
    "            \"optimizer\": optimizer,\n",
    "              \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}\n",
    "                }\n",
    "    def train_dataloader(self):\n",
    "        return self.train_dl\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return self.val_dl\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return self.test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\transformers\\models\\segformer\\feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\transformers\\utils\\deprecation.py:172: UserWarning: The following named arguments are not valid for `SegformerFeatureExtractor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n",
      "c:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\albumentations\\core\\validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "C:\\Users\\DSBG-Public\\AppData\\Local\\Temp\\ipykernel_10960\\3347461262.py:36: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([2, 256, 1, 1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\lightning_fabric\\connector.py:572: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>.\\wandb\\run-20251001_163041-7lem4ena</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/diego-gonca-simao-basel/segformer_20251001_163041/runs/7lem4ena' target=\"_blank\">Improved-aug</a></strong> to <a href='https://wandb.ai/diego-gonca-simao-basel/segformer_20251001_163041' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/diego-gonca-simao-basel/segformer_20251001_163041' target=\"_blank\">https://wandb.ai/diego-gonca-simao-basel/segformer_20251001_163041</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/diego-gonca-simao-basel/segformer_20251001_163041/runs/7lem4ena' target=\"_blank\">https://wandb.ai/diego-gonca-simao-basel/segformer_20251001_163041/runs/7lem4ena</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                             | Params | Mode\n",
      "------------------------------------------------------------------\n",
      "0 | model | SegformerForSemanticSegmentation | 3.7 M  | eval\n",
      "------------------------------------------------------------------\n",
      "3.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.7 M     Total params\n",
      "14.859    Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "213       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:01<00:01,  0.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\datasets\\features\\image.py:347: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 80/80 [02:24<00:00,  0.56it/s, v_num=4ena, train_loss=0.137, train_mIoU=0.827, train_acc=0.909]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=7` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 80/80 [02:24<00:00,  0.55it/s, v_num=4ena, train_loss=0.137, train_mIoU=0.827, train_acc=0.909]\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "feature_extractor.reduce_labels = False\n",
    "\n",
    "train_dataset = SemanticSegmentationDataset(TRAIN_IMAGE_DIR,TRAIN_MASK_DIR, feature_extractor)\n",
    "val_dataset = SemanticSegmentationDataset(VAL_IMAGE_DIR,VAL_MASK_DIR, feature_extractor)\n",
    "test_dataset = SemanticSegmentationDataset(TEST_IMAGE_DIR,TEST_MASK_DIR, feature_extractor)\n",
    "\n",
    "batch_size = 16\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "segformer_finetuner = SegformerFinetuner(\n",
    "    train_dataloader=train_dataloader, \n",
    "    val_dataloader=val_dataloader, \n",
    "    test_dataloader=test_dataloader, \n",
    "    metrics_interval=10,\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "   patience=8,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\"loss\")\n",
    "\n",
    "project_name = f\"segformer_{time.strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project=project_name,\n",
    "    name=\"Improved-aug\",         \n",
    "    log_model=True\n",
    ")    \n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=7,\n",
    "    precision=16,\n",
    "    gradient_clip_val=1.0,\n",
    "    accumulate_grad_batches=2,\n",
    "    logger=wandb_logger,  \n",
    "    check_val_every_n_epoch=1,\n",
    "    callbacks=[\n",
    "        checkpoint_callback\n",
    "    ],\n",
    "    enable_progress_bar=True \n",
    ")\n",
    "\n",
    "trainer.fit(segformer_finetuner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at .\\segformer_20251001_163041\\7lem4ena\\checkpoints\\epoch=6-step=280.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at .\\segformer_20251001_163041\\7lem4ena\\checkpoints\\epoch=6-step=280.ckpt\n",
      "c:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 30/30 [01:08<00:00,  0.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.26550668478012085    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.26550668478012085   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = trainer.test(ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1500, 1500] at entry 0 and [512, 512] at entry 11",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# ─── get one test batch ──────────────────────────────────────────\u001b[39;00m\n\u001b[0;32m     23\u001b[0m rand_batch_idx \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandrange(\u001b[38;5;28mlen\u001b[39m(visual_dataloader))\n\u001b[1;32m---> 24\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisual_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrand_batch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[0;32m     25\u001b[0m images \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]             \n\u001b[0;32m     26\u001b[0m masks  \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# CHW tensors, on CPU\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:127\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n",
      "File \u001b[1;32mc:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:127\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n",
      "File \u001b[1;32mc:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\DSBG-Public\\.conda\\envs\\Seg-Rob\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1500, 1500] at entry 0 and [512, 512] at entry 11"
     ]
    }
   ],
   "source": [
    "import torch            # 1️⃣ make sure torch is imported\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random, itertools\n",
    "\n",
    "visual_dataset = SemanticSegmentationDataset(TEST_IMAGE_DIR, TEST_MASK_DIR, feature_extractor, augment=False)\n",
    "visual_dataloader = DataLoader(visual_dataset, batch_size=16)\n",
    "# ─── colour palette ──────────────────────────────────────────────\n",
    "color_map = {0: (0, 0, 0),         # background  → black\n",
    "             1: (255, 0, 0)}       # object      → red\n",
    "\n",
    "def to_rgb(mask: torch.Tensor | np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Class‑index mask → H×W×3 RGB image.\"\"\"\n",
    "    mask = mask.cpu().numpy()          # work on numpy\n",
    "    rgb  = np.zeros(mask.shape + (3,), dtype=np.uint8)\n",
    "    for cls, col in color_map.items():\n",
    "        rgb[mask == cls] = col\n",
    "    return rgb\n",
    "\n",
    "# ─── get one test batch ──────────────────────────────────────────\n",
    "\n",
    "rand_batch_idx = random.randrange(len(visual_dataloader))\n",
    "batch = next(itertools.islice(visual_dataloader, rand_batch_idx, None))   \n",
    "images = batch[\"pixel_values\"]             \n",
    "masks  = batch[\"labels\"]  # CHW tensors, on CPU\n",
    "\n",
    "mean = torch.tensor(feature_extractor.image_mean).view(3, 1, 1)\n",
    "std  = torch.tensor(feature_extractor.image_std).view(3, 1, 1)\n",
    "\n",
    "def denorm(t: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Undo (x‑mean)/std → 0‑1 float array ready for imshow.\"\"\"\n",
    "    x = (t * std + mean).clamp(0, 1)        # CHW tensor, 0‑1\n",
    "    return x.permute(1, 2, 0).cpu().numpy() # → HWC\n",
    "\n",
    "\n",
    "with torch.no_grad():                         # 2️⃣ torch.no_grad, not ns.no_grad\n",
    "    _, logits = segformer_finetuner.model(images, masks)\n",
    "\n",
    "# up‑sample 128×128 logits → 512×512, then argmax → predicted mask\n",
    "logits = torch.nn.functional.interpolate(\n",
    "            logits, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "pred = logits.argmax(dim=1)                   # shape: B×H×W\n",
    "\n",
    "# ─── visualise first n samples ───────────────────────────────────\n",
    "k = 4                     # 3️⃣ safe slice\n",
    "sel = random.sample(range(images.size(0)), k)\n",
    "\n",
    "plt.figure(figsize=(9, 3*k))\n",
    "for i, idx in enumerate(sel):\n",
    "    plt.subplot(k,3,3*i+1); plt.imshow(denorm(images[idx]));     plt.title(\"image\");       plt.axis(\"off\")\n",
    "    plt.subplot(k,3,3*i+2); plt.imshow(to_rgb(pred[idx].cpu())); plt.title(\"prediction\");  plt.axis(\"off\")\n",
    "    plt.subplot(k,3,3*i+3); plt.imshow(to_rgb(masks[idx].cpu()));plt.title(\"ground truth\");plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Seg-Rob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
